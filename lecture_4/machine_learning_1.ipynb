{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this work?\n",
    "Modern machine learning algorithms employ a wide variety of techniques. Scaling these requires a similarly wide variety of different approaches. Generally solutions fall into the following three categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize Scikit-Learn Directly\n",
    "Scikit-Learn already provides parallel computing on a single machine with Joblib. Dask extends this parallelism to many machines in a cluster. This works well for modest data sizes but large computations, such as random forests, hyper-parameter optimization, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "client = Client()  # Connect to a Dask Cluster\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    # Your normal scikit-learn code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reimplement Scalable Algorithms with Dask Array\n",
    "Some machine learning algorithms are easy to write down as Numpy algorithms. In these cases we can replace Numpy arrays with Dask arrays to achieve scalable algorithms easily. This is employed for linear models, pre-processing, and clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models on Large Datasets\n",
    "Most estimators in scikit-learn are designed to work with NumPy arrays or scipy sparse matricies. These data structures must fit in the RAM on a single machine.\n",
    "\n",
    "Estimators implemented in Dask-ML work well with Dask Arrays and DataFrames. This can be much larger than a single machine’s RAM. They can be distributed in memory on a cluster of machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Scale up: connect to your own cluster with more resources\n",
    "# see http://dask.pydata.org/en/latest/setup.html\n",
    "client = Client('192.168.9.30:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_ml.datasets\n",
    "import dask_ml.cluster\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale up: increase n_samples or n_features\n",
    "X, y = dask_ml.datasets.make_blobs(n_samples=1000000,\n",
    "                                   chunks=100000,\n",
    "                                   random_state=0,\n",
    "                                   centers=3)\n",
    "X = X.persist()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = dask_ml.cluster.KMeans(n_clusters=3, init_max_iter=2, oversampling_factor=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[::1000, 0], X[::1000, 1], marker='.', c=km.labels_[::1000],\n",
    "           cmap='viridis', alpha=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "\n",
    "from timeit import default_timer as tic\n",
    "import sklearn.cluster\n",
    "import dask_ml.cluster\n",
    "import numpy as np\n",
    "from dask_ml.model_selection import train_test_split\n",
    "#import seaborn as sns\n",
    "\n",
    "Ns = [10000, 12000, 20000]\n",
    "\n",
    "\n",
    "timings = []\n",
    "for n in Ns:\n",
    "    X, y = make_circles(n_samples=n, random_state=n, noise=0.5, factor=0.5)\n",
    "    X[np.isnan(X)]=1\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "    \n",
    "    \n",
    "    client.scatter(X_train)\n",
    "    client.scatter(y_train)\n",
    "    client.scatter(X_test)\n",
    "    client.scatter(y_test)\n",
    "    \n",
    "    t1 = tic()\n",
    "    try:\n",
    "        clf = sklearn.cluster.SpectralClustering(n_clusters=2,n_jobs=-1).fit(X)\n",
    "        timings.append(('Scikit-Learn (exact)', n, tic() - t1))\n",
    "        print(\"Accuracy sklearn (exact): \"+str(sklearn.metrics.accuracy_score(y_test, clf.fit_predict(X_test))))\n",
    "    except MemoryError as error:\n",
    "        timings.append(('Scikit-Learn (exact)', n, np.nan))\n",
    "        print(\"Accuracy sklearn (exact): \"+str(np.nan))\n",
    "    \n",
    "    t1 = tic()\n",
    "    clf = dask_ml.cluster.SpectralClustering(n_clusters=2, n_components=6, n_jobs=-1).fit(X) ##number of cluster workers\n",
    "    timings.append(('dask-ml (approximate)', n, tic() - t1))\n",
    "    print(\"Accuracy dask-ml (approximate): \"+str(sklearn.metrics.accuracy_score(y_test, clf.fit_predict(X_test))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk about a paramter of the algorithm. From Dask docs [Dask-ml](https://dask-ml.readthedocs.io/en/latest/modules/generated/dask_ml.cluster.SpectralClustering.html)\n",
    "n_components parameter is a necessary parameter in order tu distributed-parallelize some clsutering algorithm as SpectralClustering. It corresponds to the number of rows from X to use for the Nyström approximation. \n",
    "Larger n_components will improve the accuracy of the approximation, at the cost of a longer training time.\n",
    "\n",
    "Let's back to the previuos step an take a look to the  accuracy scores. \n",
    "\n",
    "Even though cluster takes less time than standard approach and even though in some cases it can give a result while standard approach can't, looking the scores, seems that cluster's results are slightly different from standard approach. \n",
    "\n",
    "This is due to the approximations that are necessary when you work on cluster. Sometimes this approximation may improve the final results, but in general it makes them worse.\n",
    "\n",
    "Many ML algorithms, in order to work properly with distributed computation needs to make some approximations.\n",
    "\n",
    "In other words, when you have to choose between clustering and standard approach, you have to take in consideration this aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(timings, columns=['method', 'Number of Samples', 'Fit Time'])\n",
    "df.head()\n",
    "\n",
    "sk = df[df['method']=='Scikit-Learn (exact)']\n",
    "dk = df[df['method']=='dask-ml (approximate)']\n",
    "print(sk.head())\n",
    "print(dk.head())\n",
    "\n",
    "plt.plot( dk['Number of Samples'].values, dk['Fit Time'].values, color='green')\n",
    "plt.plot(sk['Number of Samples'].values,sk['Fit Time'].values, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is it happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the cluster implementation have solved each task, the non distributed computation have stopped with 20000 elments due to the high memory usage!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental Learning\n",
    "\n",
    "The incremental learning is a strategy in order to work with Big Data and distributed computation. The main idea is based on the fact that some estimators can be trained incrementally – without seeing the entire dataset at once. Scikit-Learn provdes the partial_fit API to stream batches of data to an estimator that can be fit in batches. Unfortunately, not all algorithms can be trained in this way.\n",
    "\n",
    "Normally, if you pass a Dask Array to an estimator expecting a NumPy array, the Dask Array will be converted to a single, large NumPy array. On a single machine, you’ll likely run out of RAM and crash the program. On a distributed cluster, all the workers will send their data to a single machine and crash it.\n",
    "\n",
    "```dask_ml.wrappers.Incremental``` provides a bridge between Dask and Scikit-Learn estimators supporting the ```partial_fit``` API. You wrap the underlying estimator in ```Incremental```. Dask-ML will sequentially pass each block of a Dask Array to the underlying estimator’s partial_fit method.\n",
    "\n",
    "#### Hint:\n",
    "```dask_ml.wrappers.Incremetnal``` currently does not work well with hyper-parameter optimization like ```sklearn.model_selection.GridSearchCV```\n",
    "\n",
    "### Wath are the Sklearn algorithms that support distributed computing with partial_fit?\n",
    "\n",
    "As you can find at this [link](https://scikit-learn.org/stable/modules/computing.html), the algorithms that support ```partial_fit``` are:\n",
    "\n",
    "+ Classification\n",
    "    - sklearn.naive_bayes.MultinomialNB\n",
    "    - sklearn.naive_bayes.BernoulliNB\n",
    "    - sklearn.linear_model.Perceptron\n",
    "    - sklearn.linear_model.SGDClassifier\n",
    "    - sklearn.linear_model.PassiveAggressiveClassifier\n",
    "    - sklearn.neural_network.MLPClassifier\n",
    "+ Regression\n",
    "    - sklearn.linear_model.SGDRegressor\n",
    "    - sklearn.linear_model.PassiveAggressiveRegressor\n",
    "    - sklearn.neural_network.MLPRegressor\n",
    "+ Clustering\n",
    "    - sklearn.cluster.MiniBatchKMeans\n",
    "    - sklearn.cluster.Birch\n",
    "+ Decomposition / feature Extraction\n",
    "    - sklearn.decomposition.MiniBatchDictionaryLearning\n",
    "    - sklearn.decomposition.IncrementalPCA\n",
    "    - sklearn.decomposition.LatentDirichletAllocation\n",
    "+ Preprocessing\n",
    "    - sklearn.preprocessing.StandardScaler\n",
    "    - sklearn.preprocessing.MinMaxScaler\n",
    "    - sklearn.preprocessing.MaxAbsScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to do something with MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "\n",
    "X, y = make_classification(n_samples=5000000,  n_features=20, chunks=100000, random_state=42, class_sep=10)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a simple neural network on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are a hero, brave and fearless try to launch this process with 2000 iterations. \n",
    "# When you're done tell us how much time you've spent, \n",
    "# unless you've become too old to read!\n",
    "from dask.distributed import Client\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Scale up: connect to your own cluster with more resources\n",
    "# see http://dask.pydata.org/en/latest/setup.html\n",
    "client = Client('192.168.1.9:8786')\n",
    "client\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "estimator = MLPClassifier(random_state=42, \n",
    "                          max_iter=1, \n",
    "                          hidden_layer_sizes=(200, 200 ), \n",
    "                          activation='relu', \n",
    "                          solver='adam', alpha=0.0001, \n",
    "                          batch_size='auto', \n",
    "                          learning_rate_init=0.001, \n",
    "                          shuffle=True, \n",
    "                          tol=0.0001, \n",
    "                          validation_fraction=0.1)\n",
    "t1 = tic()\n",
    "try:\n",
    "    estimator.fit(X, y)\n",
    "    t2 = tic()\n",
    "    print(\"Accuracy: \"+str(accuracy_score(y_test, estimator.predict(X_test))))\n",
    "    print(\"Time: \"+str(t2-t1)+\"s\")\n",
    "except MemoryError as error:\n",
    "    print(\"Memory error!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those that are mere mortals and are not heroes, let's see how much it uses the distributed training of a neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.cluster\n",
    "import dask_ml.cluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "from dask_ml.wrappers import Incremental\n",
    "client = Client('192.168.1.9:8786')\n",
    "client\n",
    "\n",
    "\n",
    "\n",
    "estimator = MLPClassifier(random_state=42, \n",
    "                          max_iter=2000, \n",
    "                          hidden_layer_sizes=(200, 200 ), \n",
    "                          activation='relu', \n",
    "                          solver='adam', alpha=0.0001, \n",
    "                          batch_size='auto', \n",
    "                          learning_rate_init=0.001, \n",
    "                          shuffle=True, \n",
    "                          tol=0.0001, \n",
    "                          validation_fraction=0.1)\n",
    "\n",
    "t1 = tic()\n",
    "clf = Incremental(estimator)\n",
    "clf.fit(X, y, classes=[0, 1])\n",
    "t2 = tic()\n",
    "print(\"Accuracy: \"+str(sklearn.metrics.accuracy_score(y_test, clf.predict(X_test))))\n",
    "print(\"Time: \"+str(t2-t1)+\"s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel and distributed Learning with JobLib\n",
    "\n",
    "Due to Sklearn implement multithreaded algorithms like the RandomForest one, it's possible to distribute the computation across the cluster in order to speedup the training.\n",
    "Attention! This is a differnt approach, this approach is intended to improve the speed performances not to deal with big data!\n",
    "\n",
    "Let's see an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "from dask.distributed import Client\n",
    "client = Client('192.168.1.9:8786')\n",
    "client\n",
    "X, y = make_classification(n_samples=60000,  n_features=10, chunks=10000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "client.persist(X_train)\n",
    "client.persist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dask.distributed import Client\n",
    "try:\n",
    "    t1 = tic()\n",
    "    clf = RandomForestClassifier(n_jobs=3, n_estimators=1000, max_depth=10,   random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = tic()\n",
    "    print(\"Accuracy: \"+str(accuracy_score(y_test, clf.predict(X_test))))\n",
    "    print(\"Time: \"+str(t2-t1)+\"s\") \n",
    "except MemoryError as error:\n",
    "    print(\"Memory error!!!\")\n",
    "    \n",
    "client = Client('192.168.1.9:8786')\n",
    "client\n",
    "## n_jobs = -1 means that the algorithm may use all the CPU resources available.\n",
    "## By using dask scheduler all the CPU resources become \"all the Cluster resources\"!\n",
    "with parallel_backend('dask'):\n",
    "    t1 = tic()\n",
    "    clf = RandomForestClassifier(n_jobs=-1, n_estimators=1000, max_depth=10,   random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = tic()\n",
    "    print(\"Accuracy: \"+str(accuracy_score(y_test, clf.predict(X_test))))\n",
    "    print(\"Time: \"+str(t2-t1)+\"s\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Algorithm that can be distriuted in this way is the KNeighborsClassifier (more info [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)).\n",
    "\n",
    "Let's see an exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "\n",
    "X, y = make_classification(n_samples=60000,  n_features=10, chunks=10000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n",
    "\n",
    "client.persist(X_train)\n",
    "client.persist(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "try:\n",
    "    t1 = tic()\n",
    "    clf = KNeighborsClassifier(n_jobs=1, n_neighbors=6, leaf_size=40, algorithm='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = tic()\n",
    "    print(\"Accuracy: \"+str(sklearn.metrics.accuracy_score(y_test, clf.predict(X_test))))\n",
    "    print(\"Time: \"+str(t2-t1)+\"s\") \n",
    "except MemoryError as error:\n",
    "    print(\"Memory error!!!\")\n",
    "    \n",
    "client = Client('192.168.9.30:8786')\n",
    "client\n",
    "## n_jobs = -1 means that the algorithm may use all the CPU resources available.\n",
    "## By using dask scheduler all the CPU resources become \"all the Cluster resources\"!\n",
    "with parallel_backend('dask'):\n",
    "    t1 = tic()\n",
    "    clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=6, leaf_size=40, algorithm='auto')\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = tic()\n",
    "    print(\"Accuracy: \"+str(sklearn.metrics.accuracy_score(y_test, clf.predict(X_test))))\n",
    "    print(\"Time: \"+str(t2-t1)+\"s\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "Machine learning algorithms often needs to rescale/tranform the features of your dataset elements. Sklearn contains a lot of transfomer (aka Scaler) in order to solve these tasks.\n",
    "Anyway, even thought the trasformations are easy and in the morijity of cases perform simple stastical operations, with Big data this become a huge problem.\n",
    "\n",
    "Similar to the pure ML Algorithms, even for those transformer are not completely supported. At the time of writing this lectures the available distributed transfomer  are:\n",
    "+ [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "+ [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
    "+ [MaxAbsScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)\n",
    "+ [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html)\n",
    "\n",
    "Let's try with MinMaxScaler. As reported in [SklearnDocs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html), MinMaxScaler transforms features by scaling each feature to a given range. e.g. between zero and one. The default range is 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = tic()\n",
    "transformer = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "X_train = transformer.fit_transform(X_train)\n",
    "t2 = tic()\n",
    "print(\"Time: \"+str(t2-t1)+\"s\")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with distributed computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "t1 = tic()\n",
    "transformer = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "X_train = transformer.fit_transform(X_train)\n",
    "t2 = tic()\n",
    "print(\"Time: \"+str(t2-t1)+\"s\")\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed variant have took 1/10 of the inital time!!\n",
    "Attention! Even thought those scaler implement the ```partial_fit``` method, they cannot be used with IncreamentalLerning, let's see why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import Incremental\n",
    "t1 = tic()\n",
    "transformer = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "estimator = Incremental(transformer).fit(X_train)\n",
    "X_train_inc = estimator.transform(X_train)\n",
    "t2 = tic()\n",
    "print(\"Time: \"+str(t2-t1)+\"s\")\n",
    "X_train_inc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have understand how work the distributed machine learning training and the transformation processes, let's try to merge them together an see the differences in terms of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "\n",
    "X, y = make_classification(n_samples=5000000,  n_features=20, chunks=100000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "transformer = sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "transformer.fit(X_train)\n",
    "\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test = transformer.transform(X_test)\n",
    "\n",
    "estimator = MLPClassifier(random_state=42, \n",
    "                          max_iter=1, \n",
    "                          hidden_layer_sizes=(200, 200 ), \n",
    "                          activation='relu', \n",
    "                          solver='adam', alpha=0.0001, \n",
    "                          batch_size='auto', \n",
    "                          learning_rate_init=0.001, \n",
    "                          shuffle=True, \n",
    "                          tol=0.0001, \n",
    "                          validation_fraction=0.1)\n",
    "t1 = tic()\n",
    "try:\n",
    "    estimator.fit(X, y)\n",
    "    t2 = tic()\n",
    "    print(\"Accuracy: \"+str(sklearn.metrics.accuracy_score(y_test, estimator.predict(X_test))))\n",
    "    print(\"Time: \"+str(t2-t1)+\"s\")\n",
    "except MemoryError as error:\n",
    "    print(\"Memory error!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what change with distributed computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "\n",
    "X, y = make_classification(n_samples=5000000,  n_features=20, chunks=100000, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "transformer = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "transformer.fit(X_train)\n",
    "\n",
    "X_train = transformer.transform(X_train)\n",
    "X_test = transformer.transform(X_test)\n",
    "\n",
    "estimator = MLPClassifier(random_state=42, \n",
    "                          max_iter=1, \n",
    "                          hidden_layer_sizes=(200, 200 ), \n",
    "                          activation='relu', \n",
    "                          solver='adam', alpha=0.0001, \n",
    "                          batch_size='auto', \n",
    "                          learning_rate_init=0.001, \n",
    "                          shuffle=True, \n",
    "                          tol=0.0001, \n",
    "                          validation_fraction=0.1)\n",
    "t1 = tic()\n",
    "try:\n",
    "    clf = Incremental(estimator)\n",
    "    clf.fit(X, y, classes=[0, 1])\n",
    "    t2 = tic()\n",
    "    print(\"Accuracy: \"+str(sklearn.metrics.accuracy_score(y_test, clf.predict(X_test))))\n",
    "    print(\"Time: \"+str(t2-t1)+\"s\")\n",
    "except MemoryError as error:\n",
    "    print(\"Memory error!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Prediction and Transformation\n",
    "\n",
    "In real world environment, there are some cases where you already have a trained model, but you have to predict/classify a large dataset.\n",
    "\n",
    "Dask-ml provides some meta-estimators that parallelize and scaling out certain tasks that may not be parallelized within scikit-learn itself. \n",
    "For example, ParallelPostFit will parallelize the ```predict```, ```predict_proba``` and ```transform``` methods, enabling them to work on large (possibly larger-than memory) datasets.\n",
    "\n",
    "Note that many scikit-learn estimators already predict and transform in parallel. This meta-estimator may still be useful in those cases when your dataset is larger than memory, as the distributed scheduler will ensure the data isn’t all read into memory at once.\n",
    "\n",
    "Attention!\n",
    "*This class is not appropriate for parallel or distributed training on large datasets!!*\n",
    "\n",
    "In other words, tanks to dask you can predict and trasform a large sets of data. Let's try an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import ParallelPostFit\n",
    "X, y = sklearn.datasets.make_classification(n_samples=1000, random_state=0)\n",
    "\n",
    "estimator = MLPClassifier(random_state=42, \n",
    "                          max_iter=100, \n",
    "                          hidden_layer_sizes=(200, 200 ), \n",
    "                          activation='relu', \n",
    "                          solver='adam', alpha=0.0001, \n",
    "                          batch_size='auto', \n",
    "                          learning_rate_init=0.001, \n",
    "                          shuffle=True, \n",
    "                          tol=0.0001, \n",
    "                          validation_fraction=0.1)\n",
    "estimator = estimator.fit(X, y)\n",
    "\n",
    "clf = ParallelPostFit(estimator=estimator,scoring='accuracy')\n",
    "\n",
    "X_big, y_big = dask_ml.datasets.make_classification(n_samples=100000, chunks=1000)\n",
    "\n",
    "print(\"Accuracy: \"+str(sklearn.metrics.accuracy_score(y_big, clf.predict(X_big))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's merge what we have learn together:\n",
    "\n",
    "+ Get big dataset\n",
    "+ Scale a big dataset\n",
    "+ Split the dataset in train and test\n",
    "+ Train algorithm on a big train set\n",
    "+ Predict a big test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "\n",
    "X, y = make_classification(n_samples=10000000,  n_features=20, chunks=1000000, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "transformer = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "X = transformer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MLPClassifier(random_state=42, \n",
    "                          max_iter=1000, \n",
    "                          hidden_layer_sizes=(200, 200 ), \n",
    "                          activation='relu', \n",
    "                          solver='adam', alpha=0.0001, \n",
    "                          batch_size='auto', \n",
    "                          learning_rate_init=0.001, \n",
    "                          shuffle=True, \n",
    "                          tol=0.0001, \n",
    "                          validation_fraction=0.1)\n",
    "t1 = tic()\n",
    "clf = Incremental(estimator)\n",
    "clf.fit(X, y, classes=[0, 1])\n",
    "clf = ParallelPostFit(estimator=clf.estimator_,scoring='accuracy')\n",
    "t2 = tic()\n",
    "print(\"Accuracy: \"+str(dask_ml.metrics.accuracy_score(y_test, clf.predict(X_test))))\n",
    "print(\"Time: \"+str(t2-t1)+\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Try to classify hand-written digist on the datasets. Classify the number 5 and number 9. Load the remote dataset available here: [https://www.dropbox.com/s/366wxe7458yixl4/numbers.csv?dl=1](https://www.dropbox.com/s/366wxe7458yixl4/numbers.csv?dl=1)\n",
    "\n",
    "You have to use **MLPClassifier and Incremental learning and Parallel post fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.cluster\n",
    "import dask_ml.cluster\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from dask_ml.wrappers import Incremental\n",
    "client =  #put your code\n",
    "client\n",
    "\n",
    "df =  #put your code\n",
    "\n",
    "numbers = df[df['label'].isin([5, 9])]\n",
    "\n",
    "\n",
    "y = numbers['label'].to_dask_array(lengths=True)\n",
    "X = numbers[numbers.columns.difference(['label'])].to_dask_array(lengths=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test =  #put your code\n",
    "\n",
    "estimator =  #put your code\n",
    "\n",
    "clf =  #put your code\n",
    "clf.fit(X, y, classes=[5, 9])\n",
    "clf =  #put your code\n",
    "\n",
    "print(\"Accuracy: \"+str(dask_ml.metrics.accuracy_score(y_test, clf.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Similar to the exercise 1, try to classify hand-written digist 5 and 9 on the datasets. Before the training Scale the features by using the StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.cluster\n",
    "import dask_ml.cluster\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    " #put your code\n",
    "client\n",
    "\n",
    "df =  #put your code\n",
    "\n",
    "numbers = df[df['label'].isin([5, 9])]\n",
    "numbers.head()\n",
    "\n",
    "y = numbers['label'].to_dask_array(lengths=True)\n",
    "X = numbers[numbers.columns.difference(['label'])].to_dask_array(lengths=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = #put your code\n",
    "\n",
    "transformer =  #put your code\n",
    " #put your code\n",
    "\n",
    "X_train =  #put your code\n",
    "X_test =  #put your code\n",
    "\n",
    "\n",
    "estimator =  #put your code\n",
    "\n",
    "clf =  #put your code\n",
    "clf.fit(X, y, classes=[5, 9])\n",
    "clf =  #put your code\n",
    "\n",
    "print(\"Accuracy: \"+str(dask_ml.metrics.accuracy_score(y_test, clf.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Similar to the exercise 2, try to classify hand-written 1 and 8 digist on the datasets. Before the training Scale the features by using the RobustScaler.\n",
    "Use the SGDClassifier, [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) you can find how to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import sklearn.cluster\n",
    "import dask_ml.cluster\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.preprocessing import RobustScaler\n",
    "client = #put your code\n",
    "client\n",
    "\n",
    "df = #put your code\n",
    "\n",
    "numbers = df[df['label'].isin([1, 8])]\n",
    "numbers.head()\n",
    "\n",
    "y = numbers['label'].to_dask_array(lengths=True)\n",
    "X = numbers[numbers.columns.difference(['label'])].to_dask_array(lengths=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = #put your code\n",
    "\n",
    "transformer = #put your code\n",
    "#put your code\n",
    "\n",
    "X_train = #put your code\n",
    "X_test = #put your code\n",
    "\n",
    "\n",
    "estimator = #put your code\n",
    "\n",
    "clf = #put your code\n",
    "clf.fit(X, y, classes=[1, 8])\n",
    "clf = #put your code\n",
    "\n",
    "print(\"Accuracy: \"+str(dask_ml.metrics.accuracy_score(y_test, clf.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Try to separate dogs from cats on the datasets by using the ```parallel_backend('dask')``` and the ```RandomForestClasifier```. Load the remote dataset available here: [https://www.dropbox.com/s/lo8hmhczoo6c7yo/dogs_and_cats.csv?dl=1](https://www.dropbox.com/s/lo8hmhczoo6c7yo/dogs_and_cats.csv?dl=1)\n",
    "\n",
    "Before to train use the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from dask_ml.preprocessing import MinMaxScaler\n",
    "\n",
    "df = #put your code\n",
    "client = #put your code\n",
    "client\n",
    "\n",
    "y = df['label'].to_dask_array(lengths=True)\n",
    "X = df[df.columns.difference(['label'])].to_dask_array(lengths=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)\n",
    "\n",
    "transformer = #put your code\n",
    "#put your code\n",
    "\n",
    "X_train = #put your code\n",
    "X_test = #put your code\n",
    "\n",
    "\n",
    "## n_jobs = -1 means that the algorithm may use all the CPU resources available.\n",
    "## By using dask scheduler all the CPU resources become \"all the Cluster resources\"!\n",
    "with parallel_backend('dask'):\n",
    "   \n",
    "    #put your code\n",
    "    #put your code\n",
    "\n",
    "print(\"Accuracy: \"+str(sklearn.metrics.accuracy_score(y_test, clf.predict(X_test))))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Similar to previous exercise, try to seprate cats from dogs, by using ```MLPClassifier``` and Incremental learning and Parallel post fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.cluster\n",
    "import dask_ml.cluster\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.datasets import make_classification\n",
    "from timeit import default_timer as tic\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from dask_ml.wrappers import Incremental\n",
    "from dask_ml.preprocessing import StandardScaler\n",
    "client = #put your code\n",
    "client\n",
    "\n",
    "df = #put your code\n",
    "\n",
    "\n",
    "y = df['label'].to_dask_array(lengths=True)\n",
    "X = df[df.columns.difference(['label'])].to_dask_array(lengths=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = #put your code\n",
    "\n",
    "\n",
    "\n",
    "transformer = #put your code\n",
    "transformer = #put your code\n",
    "\n",
    "X_train = #put your code\n",
    "X_test = #put your code\n",
    "\n",
    "\n",
    "estimator = #put your code\n",
    "\n",
    "\n",
    "clf = #put your code\n",
    "#put your code\n",
    "clf = #put your code\n",
    "\n",
    "print(\"Accuracy: \"+str(dask_ml.metrics.accuracy_score(y_test, clf.predict(X_test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
