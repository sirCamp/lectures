{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage in big data and distributed systems\n",
    "\n",
    "Efficient storage can dramatically improve performance, particularly when operating repeatedly from disk.\n",
    "\n",
    "Decompressing text and parsing CSV files is expensive. One of the most effective strategies with medium data is to use a binary storage format like HDF5 or Parquet. Often the performance gains from doing this is sufficient so that you can switch back to using Pandas again instead of using dask.dataframe.\n",
    "\n",
    "In this section we'll learn how to efficiently arrange and store your datasets in on-disk binary formats. We'll use the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Take-aways\n",
    "\n",
    "+ Storage formats affect performance by an order of magnitude\n",
    "+ Text data will keep even a fast format like HDF5 slow\n",
    "+ A combination of binary formats, column storage, and partitioned data turns one second wait times into 80ms wait times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filename = os.path.join('data', 'accounts.*.csv')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df_csv = dd.read_csv(filename)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = os.path.join('data', 'accounts.h5')\n",
    "target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_csv.to_hdf(target, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# same data as before\n",
    "df_hdf = dd.read_hdf(target, '/data')\n",
    "df_hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_csv.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_hdf.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly they are about the same, or perhaps even slower.\n",
    "\n",
    "The culprit here is names column, which is of object dtype and thus hard to store efficiently. There are two problems here:\n",
    "\n",
    "How do we store text data like names efficiently on disk?\n",
    "Why did we have to read the names column when all we wanted was amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = os.path.join('data', 'accounts_optimized.h5')\n",
    "%time df_hdf.categorize(columns=['names']).to_hdf(target, '/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hdf = dd.read_hdf(target, '/data')\n",
    "df_hdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# But loads more quickly\n",
    "%time df_hdf.amount.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the appropiate file extension and compression it's ncessary when you have Big data problems. \n",
    "Even though using HDF5/Parquet can use more space than a CSV when you store it on disk, when you have to do complex computation it improves the perfomances. \n",
    "This is due to the nauture of the file encoding, in other words, it depends on how the data are stored inside the file. While CSV is a text file, HDF5 files are ```binary files``` and this improve the general perfomances during the elaborations.\n",
    "\n",
    "Furthermore, HDF5 is a standard that allows to store file into \"distributed filesystem\".\n",
    "Distributed filesystems are a type of filesystems that works over LAN/WAN or internet networks.\n",
    "When you have a cluster with thousands of workers, you can create a distributed filesystem that is shared between each node. In other words, each worker node may access to the same resources.\n",
    "\n",
    "This is usefull specially when you have to store some partial computed data that should be used by different workers. In this case, in fact, a worker node X can acess to the resources generated by a worker node Y whit a reduced overhead and without store the data enterily in RAM.\n",
    "\n",
    "Even though HDF5 and Parquet are ones of the best standards for distributed file systems nowadays it's possible to store any type of file. Anyway HDF5 and Parquet formats still remain the best ones for Big data approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote files\n",
    "When you have to work with big data and clusters, you should be involved in remote resources management and loading.\n",
    "Dask can access various cloud- and cluster-oriented data storage services such as Amazon S3/HDFS/Google Cloud and Dropbox.\n",
    "\n",
    "This approach it is really usefull specially when you have to distribute the data over sperated workers that don't have a disrtibuted file system.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "+ scalable, \n",
    "+ secure storage\n",
    "\n",
    "Disadvantages:\n",
    "+ network speed becomes bottleneck\n",
    "\n",
    "The way to set up dataframes (and other collections) remains very similar to before. Note that the data here is available anonymously, but in general an extra parameter storage_options= can be passed with further details about how to interact with the remote storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to download and analyze a remote file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file allows to load data from a Amazon S3 Cloud storage, \n",
    "# unfortunately the file is larger then 2GB on our machines dont' allows to you to download it!\n",
    "# Feel free to try it on you own machine at home!\n",
    "# taxi = dd.read_csv('s3://nyc-tlc/trip data/yellow_tripdata_2015-*.csv')\n",
    "\n",
    "taxi = dd.read_csv('https://www.dropbox.com/s/17ui51hwpoqeb4p/fhv_tripdata_2015-01.csv?dl=1')\n",
    "taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How you can see from ```taxi.head()```, some data are disturbed. In this case there are some rows without a value for column ```locationID```. \n",
    "Let's remove those data and try to make a simple exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = taxi.dropna()\n",
    "taxi.head()\n",
    "taxi = taxi.repartition(npartitions=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Calculate the max number of taxi ride per location (column ```locationID```) and see the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max number of ride fo a location: \"+str(taxi.groupby('locationID').locationID.count().max().compute()))\n",
    "taxi.groupby('locationID').locationID.count().max().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Calculate the mean and the std number of taxi ride per location (column ```locationID```) without repeating the same operations. Then see the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "mean = #write your code here\n",
    "std = #write your code here\n",
    "\n",
    "print(\"The mean and std are: \"+str())\n",
    "#visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
