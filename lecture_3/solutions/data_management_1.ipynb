{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distibuted dataset computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask allow to manage large dataset through Pandas, the common python library for dataset management.\n",
    "Pandas is great for tabular datasets that fit in memory. Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Dask.dataframe?\n",
    "\n",
    "The dask.dataframe module implements a blocked parallel DataFrame object that mimics a large subset of the Pandas DataFrame. One Dask DataFrame is comprised of many in-memory pandas DataFrames separated along the index. One operation on a Dask DataFrame triggers many pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, Dask.dataframe it's really important for two reasons mainly:\n",
    "Related Documentation\n",
    "\n",
    "+ Dask.dataframe should be familiar to Pandas users\n",
    "+ The partitioning of dataframes is important for efficient queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is “partitioning” ?\n",
    "\n",
    "Repartition is a technique to reduce overhead and necessary to exploit the power of a cluster.\n",
    "Assume that you have a 2Tb dataset, in this case you may have two main problems:\n",
    "+ Probably this dataset wont fit you RAM (too much larger)\n",
    "+ The  time necessary to send 2Tb of data to each worker of the cluster is too much long, even with powerful networks\n",
    "\n",
    "Your Dask DataFrame is split up into many Pandas DataFrames. We sometimes call these “partitions”, and often the number of partitions is decided for you. For example, it might be the number of CSV files from which you are reading. However, over time, as you reduce or increase the size of your pandas DataFrames by filtering or joining, it may be wise to reconsider how many partitions you need. There is a cost to having too many or having too few.\n",
    "\n",
    "\n",
    "Partitions should fit comfortably in memory (smaller than a gigabyte) but also not be too many. Every operation on every partition takes the central scheduler a few hundred microseconds to process. \n",
    "This means that the number of partition shouldbe reasonable, smaller partitions should increase the overhead on execution context of a worker, smaller partitions may not fit the worker's RAM.\n",
    "\n",
    "In some situations is that you load lots of data into reasonably sized partitions (Dask’s defaults make decent choices). Anyway, if you filter down your dataset to only a small fraction of the original. At this point, it is wise to regroup your many small partitions into a few larger ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Dask.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "\n",
    "# start to read a series of CSV\n",
    "filename = os.path.join('data', 'accounts.*.csv')\n",
    "\n",
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "\n",
    "# load and count number of rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happened in this example?\n",
    "\n",
    "+ Dask investigated the input path and found that there are three matching files\n",
    "+ a set of jobs was intelligently created for each chunk - one per original CSV file in this case\n",
    "+ each file was loaded into a pandas dataframe, had len() applied to it\n",
    "+ the subtotals were combined to give you the final grant total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data\n",
    "Lets try this with an extract of flights in the USA across several years. This data is specific to flights out of the three airports in the New York City area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how it's structured the dataset (partitions, type of each column...)\n",
    "df \n",
    "\n",
    "#try to repartite the df and see how change\n",
    "df = df.repartition(npartitions=8)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to observe that the respresentation of the dataset contains no data. Frthermore, Dask has just done enough to read the start of the first file, and infer the column names and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look to the first elments\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this take some samples from the end of the dataset, try to run and see how happen\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What just happened?\n",
    "Unlike pandas.read_csv which reads in the entire file before inferring datatypes, dask.dataframe.read_csv only reads in a sample from the beginning of the file (or first file if using a glob). These inferred datatypes are then enforced when reading all partitions.\n",
    "\n",
    "In this case, the datatypes inferred in the sample are incorrect. The first n rows have no value for CRSElapsedTime (which pandas infers as a float), and later on turn out to be strings (object dtype). Note that Dask gives an informative error message about the mismatch. When this happens you have a few options:\n",
    "\n",
    "+ Specify dtypes directly using the dtype keyword. This is the recommended solution, as it's the least error prone (better to be explicit than implicit) and also the most performant.\n",
    "+ Increase the size of the sample keyword (in bytes)\n",
    "+ Use assume_missing to make dask assume that columns inferred to be int (which don't allow missing values) are actually floats (which do allow missing values). In our particular case this doesn't apply.\n",
    "\n",
    "In our case we'll use the first option and directly specify the dtypes of the offending columns.\n",
    "When you have dataset contains \"disturbed data\" you should always specify the type of the columns, specially if only a few columns contains data with noise. This solution is the better one for three main reasons:\n",
    "+ Dask don't have to infer the type of data with an improvement of the perfomances\n",
    "+ You can use the information that you have about the dataset, hence you can improve some operations\n",
    "+ The cluster don't have to comunicate with each worker for dataset analysis with an overhead reduction\n",
    "\n",
    "Increase the size of the sampled elements it's not always a good solution. Let's suppose that you have a 5 billion record dataset, ~5Tb of data. Now let's suppose that the first billion of records cotains noise and disturbed data. In this case you should load more than 1 billion records in order to infer the right type of the columns with a huge amount of time and a huge computationa and networks overhead.\n",
    "\n",
    "The ```assume_missing``` approach can be used in some data, when you don't know the nature of the data how data are made. Anyway, the first solution is still the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "\n",
    "#now it works!\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computations with dask.dataframe\n",
    "We compute the maximum of the DepDelay column. With just pandas, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "filenames = [\n",
    "    os.path.join('data', 'nycflights', '1993.csv'),\n",
    "    os.path.join('data', 'nycflights', '1994.csv'),\n",
    "    os.path.join('data', 'nycflights', '1998.csv'),\n",
    "    os.path.join('data', 'nycflights', '1999.csv')\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "print(filenames)\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    pdf = pd.read_csv(fn)\n",
    "    time.sleep(0.5) ##dummy for represent complex computation\n",
    "    maxes.append(pdf.DepDelay.max())\n",
    "    \n",
    "final_max = 0\n",
    "for m in maxes:\n",
    "    if m >= final_max:\n",
    "        final_max = m\n",
    "end_time = time.time()\n",
    "print(\"Times in seconds: \"+str(end_time-start_time))\n",
    "print(final_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "final_max = df.DepDelay.max().compute()\n",
    "end_time = time.time()\n",
    "print(\"Times in seconds: \"+str(end_time-start_time))\n",
    "print(final_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "\n",
    "As with dask.delayed, we need to call .compute() when we're done. Up until this point everything is lazy.\n",
    "Dask will delete intermediate results (like the full pandas dataframe for each file) as soon as possible.\n",
    "This lets us handle datasets that are larger than memory\n",
    "This means that repeated computations will have to load all of the data in each time.\n",
    "\n",
    "As with Delayed objects, you can view the underlying task graph using the ```.visualize``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "How many rows are in our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put your code here\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "In total, how many non-canceled flights were taken? Print the result and see the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non cancelled flights: \"+str(len(df[~df.Cancelled].compute())))\n",
    "\n",
    "df[~df.Cancelled].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "In total, how many non-cancelled flights were taken from each airport? Try to get the result, print it and see the computation graph\n",
    "\n",
    "#### Hints: \n",
    "use the ```groupby``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cancelled per airport: \"+str(df[~df.Cancelled].groupby('Origin').Origin.count().compute()))\n",
    "df[~df.Cancelled].groupby('Origin').Origin.count().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "What was the average departure delay from each airport? Try to get the result, print it and see the computation graph\n",
    "\n",
    "#### Hints:\n",
    "is this approach faster or slower that what we have done before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Departure mean delay for each airport: \"+str(df.groupby(\"Origin\").DepDelay.mean().compute()))\n",
    "df.groupby(\"Origin\").DepDelay.mean().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "What day of the week has the worst average departure delay? Try to get the result, print it and see the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Departure mean delay for each airport: \"+str(df.groupby(\"DayOfWeek\").DepDelay.mean().compute()))\n",
    "df.groupby(\"DayOfWeek\").DepDelay.mean().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared, reapeated and intermediate computations\n",
    "\n",
    "When computing all of the above, we sometimes did the same operation more than once. For most operations, dask.dataframe hashes the arguments (aka \"placeholder\", that we have already see in the first lecture), allowing duplicate computations to be shared, and only computed once.\n",
    "\n",
    "For example, lets compute the mean and standard deviation for departure delay of all non-canceled flights. Since dask operations are lazy, those values aren't the final results yet. They're just the recipe require to get the result.\n",
    "\n",
    "If we want to change the approach, we can compute them with two calls to compute,. In this case there is no sharing of intermediate computations with an improvement in term of speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What it is happened?\n",
    "\n",
    "Using dask.compute takes ~ 1/2 the time. This is because the task graphs for both results are merged when calling dask.compute, allowing shared operations to only be done once instead of twice. In particular, using dask.compute only does the following once:\n",
    "\n",
    "+ the calls to read_csv\n",
    "+ the filter ```(df[~df.Cancelled])```\n",
    "+ some of the necessary reductions (sum, count)\n",
    "\n",
    "Let's see the computation graph:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wath we can see is that a lot of operation have been merged and used one time. This is essential in order to work with big data and clusters.\n",
    "Avoid the repetition of operations may increase the performances of your cluster and speed up the whole process between the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between Dask.dataframe and Pandas\n",
    "\n",
    "Pandas is more mature and fully featured than dask.dataframe. If your data fits in memory then you should use Pandas. \n",
    "Event though dask.dataframe is really powerfull, it gives you a limited pandas experience when you operate on datasets that don't fit comfortably in memory.\n",
    "\n",
    "Be carefull, Dask.dataframe becomes meaningful for problems significantly larger than problems like what we have seen. Distributed data are awesome, but they should be used in a correct way!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
