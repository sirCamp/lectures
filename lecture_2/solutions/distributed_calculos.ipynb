{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed  code on cluster with DASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the parallelization, the concept of the cluster is to execute multiple operations per time. Even thoguh these approaches are similar, in this case the instructions are executed in parallel over several CPUs that reside on the computers that compose the cluster instead of the single machine. \n",
    "The main idea, in fact, is to exploit the huge computation power of the cluster given by a larger number of CPUs and a larger amount of memory than a simple machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is a powerful and simple instrument to create huge and scalable clusters and it allows to distribute the computation over the nodes that compose those clusters even with no strong knowledges on distributed calculos.\n",
    "\n",
    "A cluster is mainly composed by three types of machines:\n",
    "+ **Master machine**: master machine is the computer where the program is effectively running before to distrivuited the computation. In other terms, is the machine where you have to write your code and you have to work. Master machine communicato with Scheduler machine in order to submit the istructions to the cluster and in order to retrieve the results;\n",
    "+ **Scheduler machine**: The scheduler machine is responsible of scheduling the instructions and to distribute their over the machines that compose the cluster. In other terms, it decide how to schedule the processes.\n",
    "+ **Worker machine**: this type of machine is just an executor of instructions. A worker machine receive the instructions that it has to execute and then return the result to the scheduler.\n",
    "\n",
    "To give you a better idea in how a cluster works, think to the computation graph that we have seen in the previous notebook. In this case, the master machine create the graph that is submitted to the Scheduler machine that assing the execution of each node to a Worker machine.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Hint**\n",
    "When you work on a cluster you have to keep in mind that *normal programming* method is not a good solution. This because you have no guarantees on when the program will be executed. \n",
    "In fact, each time you submit an instruction to the cluster, the task is put in a **queue**. This Queue contains the list of all tasks/instructions that have been submitted to the cluster. \n",
    "When a worker machine has finished an instructuion and is turned free again, a task is taken from the queue and executed on the machine. This process continues until the queue is empty. In other words, when you submit an instruction to the cluster you have to wait until the task will be executed and  the result available. \n",
    "Hence,  it is tivial that this execution flow is not really suitable when you have a small instructions with no huge computation necessities, cause you need to wait results that can be potentially elaborated on your computer. In the other hand this is a powerfull method when you have to execute parallel instructions or with huge resources requests in terms of CPUs and RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a slightly modified version of the first example of the previous notebook and try to dirtibuted it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import time\n",
    "\n",
    "def increment(x):\n",
    "    print(\"I'm icrementing \"+str(x)+\" on this cluster node!\")\n",
    "    time.sleep(5)\n",
    "    return x + 1\n",
    "\n",
    "def decrement(x):\n",
    "    print(\"I'm decrementing \"+str(x)+\" on this cluster node!\")\n",
    "    time.sleep(3)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    print(\"I'm summing \"+str(x)+\" and \"+str(y)+\" on this cluster node!\")\n",
    "    time.sleep(7)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to create the cluster. In this case the cluster is already up, but just to give you and idea, in order to create a cluster you have to type the following command.\n",
    "\n",
    "+ dask-scheduler: this command must to be typed on your scheduler machine\n",
    "+ dask-worker SCHEDULER_IP:SCHEDULER_PORT : this command must to be typed on each worker machine\n",
    "\n",
    "a lot of option can be passed to each command, if like to try by yourself take a read to this link [Dask worker](http://distributed.dask.org/en/latest/worker.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step is to connect to the cluster, let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Leeve blank to setup a local cluster. Put the scheduler IP to distributed cluster.\n",
    "\n",
    "client = Client()\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to run the instructions and then to retrieve the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = delayed(increment)(1)\n",
    "y = delayed(decrement)(2)\n",
    "total = delayed(add)(x, y)\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask makes available a WebUI that allows to manage and monitor what is happening on the cluster. From this interface you can see the usage of the cluster nodes,  all the tasks that have to be processed and the general status of the cluster. The WebUI is available to the scheduler page: [Scheduler](http://test:port). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways to distribute instructions to the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Dask is a really flexible framework, it makes available other ways to distrubted tasks over a cluster.\n",
    "One of those possibilities is to ```submit``` single instructions to the cluster.\n",
    "\n",
    "Let's se how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(increment, 1)\n",
    "\n",
    "client.gather(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ```increment``` with its agurment ```1``` is submitted to the cluster. The function ```submit``` returns the execution *promise* (the ```future``` variable) of the instruction that we had submit.\n",
    "\n",
    "The ```future``` variable **not contains the real result**, but just the promise of its elaboration. To retrieve the result you have to invoke the ```gather``` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need to submit an instruction multiple time with different input parameters. This is a quite frequent scenario in in machine learning, for example when you want to optmize an algorithm for a particular dataset. The idea of this approach is to map the instruction that we want to executo to each argument.\n",
    "\n",
    "Let's see an example on how do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "future_results = client.map(increment, data)\n",
    "client.gather(future_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to the ```submit``` function, ```map``` function returns a list of *execution promises*. In other words it returns an exexution promises for each input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive results from the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already view, when you work with a Cluster the results must be waited until the end of the computation of the submitted istructuions. \n",
    "\n",
    "The role of ```gather``` function is to wait until the results are available. In other words, when you invoke this method, the execution of the program wait until the execution promise is effectively satisfied and its result is available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you submit an instruction to the cluster, you can see the progress bar that show the progress of the sumitted job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait, progress\n",
    "\n",
    "future = client.submit(increment, 1)\n",
    "\n",
    "display(progress(future))\n",
    "\n",
    "client.gather(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have multiple instruction that have to be submitted to the cluster where are present some results depedencies, the ```gather``` function can be invoked at the end of the program. \n",
    "This means that you can submit to the cluster a task that takes as argument an execution promise of an instruction that has been previosly submitted. \n",
    "\n",
    "In other terms, ```gather``` function can be invoked only when the results have to effectively retrived from cluster in order to manipulate it.\n",
    "\n",
    "Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = client.submit(increment, 1)\n",
    "y = client.submit(decrement, 2)\n",
    "total = client.submit(add, x, y)\n",
    "\n",
    "print(total)     # This is still the execution promise\n",
    "client.gather(total)  # This is the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**\n",
    "\n",
    "This method is necessary to work in the right way with cluster, but some frameworks, as Apache Spark, that we're going to see in last lecture, they automatically retrieve the results without manually invoke this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Distribute a for-loop code with control flow\n",
    "\n",
    "Let's go back to the exercise that we have seen previously. Try to parallelize the orginal code by distributing the instructions to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x):\n",
    "    time.sleep(4)\n",
    "    return 2 * x\n",
    "\n",
    "def is_even(x):\n",
    "    return not x % 2\n",
    "\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Sequential code\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    if is_even(x):\n",
    "        y = double(x)\n",
    "    else:\n",
    "        y = increment(x)\n",
    "    results.append(y)\n",
    "    \n",
    "total = sum(results)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the blank space by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "for x in data:\n",
    "    if is_even(x):  # even\n",
    "        y = client.submit(double, x)\n",
    "    else:          # odd\n",
    "        y = client.submit(increment, x)\n",
    "    results.append(y)\n",
    "    \n",
    "result = client.submit(sum, results)\n",
    "total = client.gather(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Count how many words are present in a series of documents \n",
    "\n",
    "What you have to do is to count how many words are present in each document. Document are ~8200. This is classical example in data analisys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n",
    "categories = [\n",
    "     'comp.graphics',\n",
    "     'comp.os.ms-windows.misc',\n",
    "     'comp.sys.ibm.pc.hardware',\n",
    "     'comp.sys.mac.hardware',\n",
    "     'comp.windows.x',\n",
    "     'misc.forsale',\n",
    "     'rec.autos',\n",
    "     'rec.motorcycles',\n",
    "     'rec.sport.baseball',\n",
    "     'rec.sport.hockey',\n",
    "     'sci.crypt',\n",
    "     'sci.electronics',\n",
    "     'sci.med',\n",
    "     'sci.space'\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories ).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Texts document present on the dataset: \"+str(len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_in_statement(text):\n",
    "    \"\"\"\n",
    "    This function takes a text as input and return the number of the words that it contains\n",
    "    \"\"\"\n",
    "    splitted_words = text.split()\n",
    "    #ime.sleep(0.5)\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "futures = client.map(count_word_in_statement, dataset)\n",
    "results = client.gather(futures)\n",
    "for index in range(0, len(results)):\n",
    "    print(\"Text #\"+str(index+1)+\" contains \"+str(results[index])+\" words\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Calculate the first *n* Fibonacci numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is the ```sequential``` version of  the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "def append(arr = [], val = 0):\n",
    "    if val != None:\n",
    "        arr.append(val)\n",
    "    return arr\n",
    "\n",
    "def fibonacci_sequential(num):\n",
    "    i = 1\n",
    "    if num == 0:\n",
    "        fibonacci = []\n",
    "    elif num == 1:\n",
    "        fibonacci = []\n",
    "        fibonacci.append(1)\n",
    "    elif num == 2:\n",
    "        fibonacci = []\n",
    "        fibonacci.append(1)\n",
    "        fibonacci.append(1)\n",
    "    elif num > 2:\n",
    "        fibonacci = []\n",
    "        fibonacci.append(1)\n",
    "        fibonacci.append(1)\n",
    "        while i < (num - 1):\n",
    "            fibonacci.append(fibonacci[i] + fibonacci[i-1])\n",
    "            i += 1\n",
    "    return fibonacci\n",
    "\n",
    "result = fibonacci_sequential(5)\n",
    "print(\"The first n fibonacci numbers are: \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can find the partial distributed code. Complete the blank spaces by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "\n",
    "def append(arr = [], val = 0):\n",
    "    #time.sleep(3)\n",
    "    if val != None:\n",
    "        arr.append(val)\n",
    "    return arr\n",
    "\n",
    "def fibonacci(num):\n",
    "    i = 1\n",
    "    if num == 0:\n",
    "        future = delayed(append)([], None)\n",
    "    elif num == 1:\n",
    "        future = delayed(append)([], 1)\n",
    "    elif num == 2:\n",
    "        future_0 = delayed(append)([], 1)\n",
    "        future = delayed(append)(future_0, 1)\n",
    "    elif num > 2:\n",
    "        future_0 = delayed(append)([], 1)\n",
    "        future = delayed(append)(future_0, 1)\n",
    "        while i < (num - 1):\n",
    "            future = delayed(append)(future, future[i] + future[i-1])\n",
    "            #fib.append(fib[i] + fib[i-1])\n",
    "            i += 1\n",
    "    return future\n",
    "\n",
    "#print(fibonacci(3).compute(scheduler='single-threaded', rerun_exceptions_locally=True, ))\n",
    "future = fibonacci(5)\n",
    "display(future.visualize(rankdir=\"LR\"))\n",
    "print(future.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Count how many values are between 0 and 10 there are in each row of a random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## creation of a matrix of 100 rows and 10 columns with each value between 0 and 100.\n",
    "np.random.RandomState(42)\n",
    "arr = np.random.randint(0, 100, size=[100, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howmany_within_range(row):\n",
    "    \"\"\"Returns how many numbers lie within `maximum` and `minimum` in a given `row`\"\"\"\n",
    "    count = 0\n",
    "    for n in row:\n",
    "        if 0 <= n <= 10:\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "\n",
    "futures = client.map(howmany_within_range, arr)\n",
    "resutls = client.gather(futures)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Estimating π using Monte Carlo integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. It is a particular Monte Carlo method that numerically computes a definite integral ( [Wiki](https://en.wikipedia.org/wiki/Monte_Carlo_integration) ).\n",
    "\n",
    "In mathematics it is possibile to estimate a value of Pi using the this method. The idea is to generate a large number of random points and see how many fall in the circle enclosed by the unit square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is a ```sequential``` version of Pi estimation with Monte Carlo integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints** value of ```inside_or_out_side``` is 1 if the point is *inside the circle* 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10000\n",
    "\n",
    "def pi_python_sequential(n):\n",
    "    points = []\n",
    "    for i in range(0, n):\n",
    "        inside_or_out_side = 0\n",
    "        x2 = random.random()**2\n",
    "        y2 = random.random()**2\n",
    "        if math.sqrt(x2 + y2) < 1.0:\n",
    "            inside_or_out_side = 1\n",
    "        \n",
    "        points.append(inside_or_out_side)\n",
    "    \n",
    "    return np.multiply(np.divide(float(sum(points)), n), 4)\n",
    "\n",
    "pi = pi_python_sequential(n)\n",
    "print(\"Estimation of Pi: \"+str(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wath you have to do in this exercise is to revirete the original code in order to distribute the computation over the cluster nodes.\n",
    "Let's start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leeve blank to setup a local cluster. Put the scheduler IP to distributed cluster.\n",
    "client = Client('192.168.9.30:8786')\n",
    "client.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 10 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/distributed/worker.py:2791: UserWarning: Large object of size 6.30 MB detected in task graph: \n",
      "  (['partial_monte_carlo-2bad7dfd-f1a9-4f19-972f-a25 ... e87325c2a25'],)\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of Pi: 3.1454\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "n = 100000\n",
    "def partial_monte_carlo():\n",
    "    inside_out_side = 0 \n",
    "    \n",
    "    x2 = random.random()**2\n",
    "    y2 = random.random()**2\n",
    "    if math.sqrt(x2 + y2) < 1.0:\n",
    "        inside_out_side = 1\n",
    "    return inside_out_side\n",
    "\n",
    "def pi_python(n):\n",
    "    points=[]\n",
    "    \n",
    "    for i in range(n):\n",
    "        points.append(delayed(partial_monte_carlo)())\n",
    "    \n",
    "    result = delayed(np.multiply)(delayed(np.divide)(delayed(float)(delayed(sum)(points)), n), 4)\n",
    "    \n",
    "    return result\n",
    "\n",
    "estimation = pi_python(n);\n",
    "print(\"Estimation of Pi: \"+str(estimation.compute()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change the value of n (number of the generated points) and see how the the loading of the cluster of the page change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
